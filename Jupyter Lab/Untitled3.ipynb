{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "581364f3-a7ca-4af3-825e-dbf4ad29fc58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean MI: 0.16363906899627848\n",
      "Standard Deviation of MI: 0.17290683961371273\n",
      "Z > 2 Threshold: 0.5094527482237039\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the mutual information matrix\n",
    "mi_matrix_path = 'Correlation/2/mutual_information2.csv'  # Adjust the path as necessary\n",
    "mi_matrix = pd.read_csv(mi_matrix_path, index_col=0)\n",
    "\n",
    "# Flatten the matrix to get mutual information values\n",
    "mi_values = mi_matrix.values.flatten()\n",
    "mi_values = mi_values[~np.isnan(mi_values)]  # Remove NaN values\n",
    "\n",
    "# Calculate mean and standard deviation\n",
    "mean_mi = np.mean(mi_values)\n",
    "std_mi = np.std(mi_values)\n",
    "\n",
    "# Calculate Z > 2 threshold\n",
    "z_threshold_2 = mean_mi + 2 * std_mi\n",
    "\n",
    "print(f'Mean MI: {mean_mi}')\n",
    "print(f'Standard Deviation of MI: {std_mi}')\n",
    "print(f'Z > 2 Threshold: {z_threshold_2}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ca062d9-8b8e-48b8-8439-cdd5770cd082",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges with threshold 0.5: 18\n",
      "90th Percentile threshold: 0.3259455280319236\n",
      "Number of edges with 90th Percentile threshold: 82\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the mutual information matrix\n",
    "mi_matrix_path = 'Correlation/2/mutual_information2.csv'  # Adjust the path as necessary\n",
    "mi_matrix = pd.read_csv(mi_matrix_path, index_col=0)\n",
    "\n",
    "# Apply the threshold of 0.80 to determine the edges\n",
    "threshold_80 = 0.50\n",
    "edges_80 = (mi_matrix.values > threshold_80).sum() - len(mi_matrix)+1  # Exclude diagonal elements\n",
    "edges_80 = edges_80 / 2  # Each edge is counted twice, so divide by 2\n",
    "\n",
    "# Flatten the matrix to get mutual information values\n",
    "mi_values = mi_matrix.values.flatten()\n",
    "mi_values = mi_values[~np.isnan(mi_values)]  # Remove NaN values\n",
    "\n",
    "# Calculate the 90th percentile threshold\n",
    "percentile_90 = np.percentile(mi_values, 90)\n",
    "edges_90th_percentile = (mi_matrix.values > percentile_90).sum() - len(mi_matrix)  # Exclude diagonal elements\n",
    "edges_90th_percentile = edges_90th_percentile / 2  # Each edge is counted twice, so divide by 2\n",
    "\n",
    "print(f'Number of edges with threshold {threshold_80}: {int(edges_80)}')\n",
    "print(f'90th Percentile threshold: {percentile_90}')\n",
    "print(f'Number of edges with 90th Percentile threshold: {int(edges_90th_percentile)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3695967-e370-416b-8b5c-2bebbbbf84db",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Edges with threshold 0.3259455280319236 saved to Correlation/2/graph_edges.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def generate_graph_edges(mi_matrix_path, output_path, threshold):\n",
    "    # Load the mutual information matrix\n",
    "    mi_matrix = pd.read_csv(mi_matrix_path, index_col=0)\n",
    "    \n",
    "    # Find the edges that meet the threshold\n",
    "    edges = []\n",
    "    for i in range(mi_matrix.shape[0]):\n",
    "        for j in range(i + 1, mi_matrix.shape[1]):\n",
    "            if mi_matrix.iloc[i, j] > threshold:\n",
    "                edges.append((mi_matrix.index[i], mi_matrix.columns[j], mi_matrix.iloc[i, j]))\n",
    "    \n",
    "    # Convert edges to a DataFrame\n",
    "    edges_df = pd.DataFrame(edges, columns=['Source', 'Target', 'Weight'])\n",
    "    \n",
    "    # Save the edges to a CSV file\n",
    "    edges_df.to_csv(output_path, index=False)\n",
    "    print(f'Edges with threshold {threshold} saved to {output_path}')\n",
    "\n",
    "# Define the paths and threshold\n",
    "mi_matrix_path = 'Correlation/2/mutual_information2.csv'  # Adjust the path as necessary\n",
    "output_path = 'Correlation/2/graph_edges.csv'  # Adjust the output path as necessary\n",
    "threshold = 0.3259455280319236  # Set your dynamic threshold here\n",
    "\n",
    "# Generate graph edges\n",
    "generate_graph_edges(mi_matrix_path, output_path, threshold)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e4fd9e-3560-4f40-a533-8ea01e8c8241",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Define the enhanced GNN model with more layers and units\n",
    "class EnhancedGNN(torch.nn.Module):\n",
    "    def __init__(self, num_node_features, num_classes):\n",
    "        super(EnhancedGNN, self).__init__()\n",
    "        self.conv1 = GCNConv(num_node_features, 64)\n",
    "        self.conv2 = GCNConv(64, 128)\n",
    "        self.conv3 = GCNConv(128, 64)\n",
    "        self.conv4 = GCNConv(64, num_classes)\n",
    "        self.dropout = torch.nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.conv4(x, edge_index)\n",
    "        # Pooling layer to get a graph-level output\n",
    "        x = global_mean_pool(x, batch)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Load edge data from CSV\n",
    "def load_edge_data(edge_file_path):\n",
    "    edge_df = pd.read_csv(edge_file_path)\n",
    "    edges = torch.tensor(edge_df[['source', 'target']].values.T, dtype=torch.long)\n",
    "    weights = torch.tensor(edge_df['weight'].values, dtype=torch.float)\n",
    "    return edges, weights\n",
    "\n",
    "# Load node data from CSV and create PyTorch Geometric Data objects\n",
    "def load_node_data(node_file_path, edge_index, edge_weight):\n",
    "    node_df = pd.read_csv(node_file_path)\n",
    "    graphs = []\n",
    "    scaler = StandardScaler()\n",
    "    for graph_id in node_df.index:\n",
    "        node_values = node_df.loc[graph_id].values.reshape(-1, 1)\n",
    "        x = torch.tensor(scaler.fit_transform(node_values), dtype=torch.float)\n",
    "        # Add the target value as a separate node without edges\n",
    "        target_value = node_df.loc[graph_id, 'target_value']\n",
    "        target_node = torch.tensor([[target_value]], dtype=torch.float)\n",
    "        x = torch.cat((x, target_node), dim=0)\n",
    "\n",
    "        # Update the edge_index and edge_weight to include the target node (no edges)\n",
    "        edge_index_with_target = edge_index.clone()\n",
    "        edge_weight_with_target = edge_weight.clone()\n",
    "\n",
    "        y = torch.tensor([graph_id], dtype=torch.long)  # Replace with actual labels if available\n",
    "        data = Data(x=x, edge_index=edge_index_with_target, edge_attr=edge_weight_with_target, y=y)\n",
    "        data.batch = torch.tensor([graph_id] * len(node_values), dtype=torch.long)  # Batch assignment\n",
    "        graphs.append(data)\n",
    "    return graphs\n",
    "\n",
    "# Define paths to the data files\n",
    "node_file_path = \"CSVs/sample_train_100.csv\"\n",
    "edge_file_path = \"Correlation/2/graph_edges.csv\"\n",
    "\n",
    "# Load the edge and node data\n",
    "edge_index, edge_weight = load_edge_data(edge_file_path)\n",
    "graphs = load_node_data(node_file_path, edge_index, edge_weight)\n",
    "\n",
    "# Split data into train and test sets (80% train, 20% test)\n",
    "train_size = int(0.8 * len(graphs))\n",
    "test_size = len(graphs) - train_size\n",
    "train_data, test_data = torch.utils.data.random_split(graphs, [train_size, test_size])\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 200\n",
    "batch_size = 16\n",
    "learning_rate = 0.001\n",
    "weight_decay = 1e-4\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the model, optimizer, and loss function\n",
    "num_node_features = 1  # Assuming each node has a single feature\n",
    "num_classes = len(set(node_df.index))  # Assuming graph IDs are used as class labels\n",
    "model = EnhancedGNN(num_node_features, num_classes)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=10, min_lr=1e-6)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Checkpointing function\n",
    "def save_checkpoint(epoch, model, optimizer, scheduler, train_losses, train_rmse_scores, test_rmse_scores, train_accuracies, test_accuracies, checkpoint_path):\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        'train_losses': train_losses,\n",
    "        'train_rmse_scores': train_rmse_scores,\n",
    "        'test_rmse_scores': test_rmse_scores,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'test_accuracies': test_accuracies,\n",
    "    }, checkpoint_path)\n",
    "\n",
    "def load_checkpoint(checkpoint_path, model, optimizer, scheduler):\n",
    "    if os.path.exists(checkpoint_path):\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "        epoch = checkpoint['epoch']\n",
    "        train_losses = checkpoint['train_losses']\n",
    "        train_rmse_scores = checkpoint['train_rmse_scores']\n",
    "        test_rmse_scores = checkpoint['test_rmse_scores']\n",
    "        train_accuracies = checkpoint['train_accuracies']\n",
    "        test_accuracies = checkpoint['test_accuracies']\n",
    "        logging.info(f'Loaded checkpoint from {checkpoint_path}, starting at epoch {epoch + 1}')\n",
    "        return epoch, train_losses, train_rmse_scores, test_rmse_scores, train_accuracies, test_accuracies\n",
    "    else:\n",
    "        return 0, [], [], [], [], []\n",
    "\n",
    "# Training function\n",
    "def train(model, train_loader, test_loader, criterion, optimizer, scheduler, num_epochs, checkpoint_path):\n",
    "    start_epoch, train_losses, train_rmse_scores, test_rmse_scores, train_accuracies, test_accuracies = load_checkpoint(checkpoint_path, model, optimizer, scheduler)\n",
    "\n",
    "    for epoch in range(start_epoch, num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct_train = 0\n",
    "        for data in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, data.y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item() * data.num_graphs\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct_train += (pred == data.y).sum().item()\n",
    "\n",
    "        train_loss = total_loss / len(train_loader.dataset)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        train_rmse, train_acc = evaluate(model, train_loader)\n",
    "        train_rmse_scores.append(train_rmse)\n",
    "        train_accuracies.append(train_acc)\n",
    "        \n",
    "        test_rmse, test_acc = evaluate(model, test_loader)\n",
    "        test_rmse_scores.append(test_rmse)\n",
    "        test_accuracies.append(test_acc)\n",
    "\n",
    "        logging.info(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train RMSE: {train_rmse:.4f}, Test RMSE: {test_rmse:.4f}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "\n",
    "        scheduler.step(train_loss)\n",
    "\n",
    "        # Save checkpoint\n",
    "        save_checkpoint(epoch, model, optimizer, scheduler, train_losses, train_rmse_scores, test_rmse_scores, train_accuracies, test_accuracies, checkpoint_path)\n",
    "\n",
    "        # Save the model at the end of each epoch\n",
    "        torch.save(model.state_dict(), os.path.join('Graphs/Graph50', 'best_model.pt'))\n",
    "\n",
    "    return train_losses, train_rmse_scores, test_rmse_scores, train_accuracies, test_accuracies\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    targets = []\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            targets.extend(data.y.tolist())\n",
    "            predictions.extend(pred.tolist())\n",
    "\n",
    "    mse_score = mean_squared_error(targets, predictions)\n",
    "    rmse_score = np.sqrt(mse_score)\n",
    "    accuracy = accuracy_score(targets, predictions)\n",
    "    return rmse_score, accuracy\n",
    "\n",
    "# Define checkpoint path\n",
    "checkpoint_path = os.path.join('Graphs/Graph50', 'checkpoint.pt')\n",
    "\n",
    "# Train the model\n",
    "train_losses, train_rmse_scores, test_rmse_scores, train_accuracies, test_accuracies = train(model, train_loader, test_loader, criterion, optimizer, scheduler, num_epochs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (env4)",
   "language": "python",
   "name": "env4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
